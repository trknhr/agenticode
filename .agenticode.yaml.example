# agenticode configuration file example
# Copy this to ~/.agenticode.yaml and update with your settings

# Provider definitions - Configure multiple LLM providers
providers:
  # OpenAI provider
  openai:
    type: openai
    base_url: https://api.openai.com/v1
    api_key: $OPENAI_API_KEY  # Uses environment variable
    models:
      - id: gpt-4-turbo-preview
        name: GPT-4 Turbo Preview
        context_window: 128000
        max_tokens: 4096
      - id: gpt-3.5-turbo
        name: GPT-3.5 Turbo
        context_window: 16385
        max_tokens: 4096
  
  # DeepSeek provider (OpenAI-compatible)
  deepseek:
    type: openai
    base_url: https://api.deepseek.com/v1
    api_key: $DEEPSEEK_API_KEY
    models:
      - id: deepseek-chat
        name: DeepSeek V3
        context_window: 64000
        max_tokens: 5000
  
  # Groq provider (OpenAI-compatible)
  groq:
    type: openai
    base_url: https://api.groq.com/openai/v1
    api_key: $GROQ_API_KEY
    models:
      - id: llama3-8b-8192
        name: Llama 3 8B
        context_window: 8192
        max_tokens: 8192
      - id: mixtral-8x7b-32768
        name: Mixtral 8x7B
        context_window: 32768
        max_tokens: 32768
  
  # Local LiteLLM proxy
  local:
    type: openai
    base_url: http://127.0.0.1:4000
    api_key: ""  # Not needed for local proxy
    models:
      - id: kimi-k2-instruct
        name: Kimi K2 Instruct
        context_window: 128000
        max_tokens: 8192

# Model selection - Define which models to use
models:
  # Default model for general use
  default: 
    provider: deepseek
    model: deepseek-chat
  
  # Fast model for quick responses
  fast:
    provider: groq
    model: llama3-8b-8192
  
  # Powerful model for complex tasks
  powerful:
    provider: openai
    model: gpt-4-turbo-preview
  
  # Summarization model - cheaper/faster model for conversation compression
  summarize:
    provider: openai
    model: gpt-3.5-turbo
  
  # Local model for offline use
  local:
    provider: local
    model: kimi-k2-instruct

# Legacy OpenAI configuration (for backwards compatibility)
# Will be used if providers section is not defined
openai:
  api_key: $OPENAI_API_KEY
  model: gpt-4-turbo-preview

# MCP (Model Context Protocol) server configurations
# Configure external tools via MCP servers
mcp:
  # Example: Filesystem MCP server
  # filesystem:
  #   type: stdio
  #   command: npx
  #   args: [-y, @modelcontextprotocol/server-filesystem, /tmp]
  #   env: {}
  #   disabled: false
  
  # Example: GitHub MCP server  
  # github:
  #   type: http
  #   url: https://api.github.com/mcp
  #   headers:
  #     Authorization: Bearer $GITHUB_TOKEN
  #   disabled: false

# Alternatively, load MCP config from a separate file
# mcp_config_file: .agenticode.mcp.yaml

# General settings
general:
  max_steps: 10                        # Maximum steps for agent execution
  confirm_before_write: true           # Ask for confirmation before writing files

# Hooks configuration
# Hooks execute commands at various points in the agent lifecycle
# hooks:
#   # Before tool execution
#   PreToolUse:
#     - matcher: "write_file|edit"     # Match write_file or edit tools
#       hooks:
#         - type: command
#           command: "echo 'About to modify files' >> ~/.agenticode/hooks.log"
#   
#   # After tool execution
#   PostToolUse:
#     - matcher: "run_shell"           # Match run_shell tool
#       hooks:
#         - type: command
#           command: "$AGENTICODE_PROJECT_DIR/.agenticode/hooks/check-shell.sh"
#           timeout: 30                # Optional timeout in seconds
#   
#   # When user submits a prompt
#   UserPromptSubmit:
#     - hooks:
#         - type: command
#           command: "date '+Context: Current time is %Y-%m-%d %H:%M:%S'"
#   
#   # When agent completes
#   Stop:
#     - hooks:
#         - type: command
#           command: "echo 'Agent completed' | notify-send"
